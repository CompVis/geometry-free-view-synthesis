{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "braindance.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgIaFMaff4Rb"
      },
      "source": [
        "# Geometry-Free View Synthesis\n",
        "\n",
        "This is a colab demo for [Geometry-Free View Synthesis](https://github.com/CompVis/geometry-free-view-synthesis). Compared to [the pygame demo](https://github.com/CompVis/geometry-free-view-synthesis#demo) the controls of this one are a bit clumsy. But you can dive right in by selecting `Runtime->Run all`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN7hjuNKcnT0"
      },
      "source": [
        "Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAm5fMd3avVn"
      },
      "source": [
        "!pip install git+https://github.com/CompVis/geometry-free-view-synthesis#egg=geometry-free-view-synthesis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ValMhAickzc"
      },
      "source": [
        "Image loading function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9YgbhaMbwCq"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def load_im_as_example(im):\n",
        "    size = [208, 368]\n",
        "    w,h = im.size\n",
        "    if np.abs(w/h - size[1]/size[0]) > 0.1:\n",
        "        print(f\"Center cropping image to AR {size[1]/size[0]}\")\n",
        "        if w/h < size[1]/size[0]:\n",
        "            # crop h\n",
        "            left = 0\n",
        "            right = w\n",
        "            top = h/2 - size[0]/size[1]*w/2\n",
        "            bottom = h/2 + size[0]/size[1]*w/2\n",
        "        else:\n",
        "            # crop w\n",
        "            top = 0\n",
        "            bottom = h\n",
        "            left = w/2 - size[1]/size[0]*h\n",
        "            right = w/2 + size[1]/size[0]*h\n",
        "        im = im.crop(box=(left, top, right, bottom))\n",
        "\n",
        "    im = im.resize((size[1],size[0]),\n",
        "                   resample=Image.LANCZOS)\n",
        "    im = np.array(im)/127.5-1.0\n",
        "    im = im.astype(np.float32)\n",
        "\n",
        "    example = dict()\n",
        "    example[\"src_img\"] = im\n",
        "    example[\"K\"] = np.array([[184.0, 0.0, 184.0],\n",
        "                             [0.0, 184.0, 104.0],\n",
        "                             [0.0, 0.0, 1.0]], dtype=np.float32)\n",
        "    example[\"K_inv\"] = np.linalg.inv(example[\"K\"])\n",
        "\n",
        "    ## dummy data not used during inference\n",
        "    example[\"dst_img\"] = np.zeros_like(example[\"src_img\"])\n",
        "    example[\"src_points\"] = np.zeros((1,3), dtype=np.float32)\n",
        "\n",
        "    return example\n",
        "\n",
        "def load_as_example(path):\n",
        "    im = Image.open(path)\n",
        "    return load_im_as_example(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzcR5qRLcaBw"
      },
      "source": [
        "Define some functions related to the camera control."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXlGobz5brB8"
      },
      "source": [
        "def normalize(x):\n",
        "    return x/np.linalg.norm(x)\n",
        "\n",
        "def cosd(x):\n",
        "    return np.cos(np.deg2rad(x))\n",
        "\n",
        "def sind(x):\n",
        "    return np.sin(np.deg2rad(x))\n",
        "\n",
        "def look_to(camera_pos, camera_dir, camera_up):\n",
        "  camera_right = normalize(np.cross(camera_up, camera_dir))\n",
        "  R = np.zeros((4, 4))\n",
        "  R[0,0:3] = normalize(camera_right)\n",
        "  R[1,0:3] = normalize(np.cross(camera_dir, camera_right))\n",
        "  R[2,0:3] = normalize(camera_dir)\n",
        "  R[3,3] = 1\n",
        "  trans_matrix = np.array([[1.0, 0.0, 0.0, -camera_pos[0]],\n",
        "                           [0.0, 1.0, 0.0, -camera_pos[1]],\n",
        "                           [0.0, 0.0, 1.0, -camera_pos[2]],\n",
        "                           [0.0, 0.0, 0.0,            1.0]])\n",
        "  tmp = R@trans_matrix\n",
        "  return tmp[:3,:3], tmp[:3,3]\n",
        "\n",
        "def rotate_around_axis(angle, axis):\n",
        "    axis = normalize(axis)\n",
        "    rotation = np.array([[cosd(angle)+axis[0]**2*(1-cosd(angle)),\n",
        "                          axis[0]*axis[1]*(1-cosd(angle))-axis[2]*sind(angle),\n",
        "                          axis[0]*axis[2]*(1-cosd(angle))+axis[1]*sind(angle)],\n",
        "                         [axis[1]*axis[0]*(1-cosd(angle))+axis[2]*sind(angle),\n",
        "                          cosd(angle)+axis[1]**2*(1-cosd(angle)),\n",
        "                          axis[1]*axis[2]*(1-cosd(angle))-axis[0]*sind(angle)],\n",
        "                         [axis[2]*axis[0]*(1-cosd(angle))-axis[1]*sind(angle),\n",
        "                          axis[2]*axis[1]*(1-cosd(angle))+axis[0]*sind(angle),\n",
        "                          cosd(angle)+axis[2]**2*(1-cosd(angle))]])\n",
        "    return rotation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPd6dlYtc01X"
      },
      "source": [
        "Forward splatting of an image given its depth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1iYAo5Zc3sw"
      },
      "source": [
        "import torch\n",
        "from splatting import splatting_function\n",
        "\n",
        "def render_forward(src_ims, src_dms,\n",
        "                   Rcam, tcam,\n",
        "                   K_src,\n",
        "                   K_dst):\n",
        "    Rcam = Rcam.to(device=src_ims.device)[None]\n",
        "    tcam = tcam.to(device=src_ims.device)[None]\n",
        "\n",
        "    R = Rcam\n",
        "    t = tcam[...,None]\n",
        "    K_src_inv = K_src.inverse()\n",
        "\n",
        "    assert len(src_ims.shape) == 4\n",
        "    assert len(src_dms.shape) == 3\n",
        "    assert src_ims.shape[1:3] == src_dms.shape[1:3], (src_ims.shape,\n",
        "                                                      src_dms.shape)\n",
        "\n",
        "    x = np.arange(src_ims[0].shape[1])\n",
        "    y = np.arange(src_ims[0].shape[0])\n",
        "    coord = np.stack(np.meshgrid(x,y), -1)\n",
        "    coord = np.concatenate((coord, np.ones_like(coord)[:,:,[0]]), -1) # z=1\n",
        "    coord = coord.astype(np.float32)\n",
        "    coord = torch.as_tensor(coord, dtype=K_src.dtype, device=K_src.device)\n",
        "    coord = coord[None] # bs, h, w, 3\n",
        "\n",
        "    D = src_dms[:,:,:,None,None]\n",
        "\n",
        "    points = K_dst[None,None,None,...]@(R[:,None,None,...]@(D*K_src_inv[None,None,None,...]@coord[:,:,:,:,None])+t[:,None,None,:,:])\n",
        "    points = points.squeeze(-1)\n",
        "\n",
        "    new_z = points[:,:,:,[2]].clone().permute(0,3,1,2) # b,1,h,w\n",
        "    points = points/torch.clamp(points[:,:,:,[2]], 1e-8, None)\n",
        "\n",
        "    src_ims = src_ims.permute(0,3,1,2)\n",
        "    flow = points - coord\n",
        "    flow = flow.permute(0,3,1,2)[:,:2,...]\n",
        "\n",
        "    alpha = 0.5\n",
        "    importance = alpha/new_z\n",
        "    importance_min = importance.amin((1,2,3),keepdim=True)\n",
        "    importance_max = importance.amax((1,2,3),keepdim=True)\n",
        "    importance=(importance-importance_min)/(importance_max-importance_min+1e-6)*10-10\n",
        "    importance = importance.exp()\n",
        "\n",
        "    input_data = torch.cat([importance*src_ims, importance], 1)\n",
        "    output_data = splatting_function(\"summation\", input_data, flow)\n",
        "\n",
        "    num = torch.sum(output_data[:,:-1,:,:], dim=0, keepdim=True)\n",
        "    nom = torch.sum(output_data[:,-1:,:,:], dim=0, keepdim=True)\n",
        "\n",
        "    rendered = num/(nom+1e-7)\n",
        "    rendered = rendered.permute(0,2,3,1)[0,...]\n",
        "    return rendered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9grU0flhdbbQ"
      },
      "source": [
        "Helper class to render with GeoGPT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgS9UpzMdQGm"
      },
      "source": [
        "from geofree import pretrained_models\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "class Renderer(object):\n",
        "    def __init__(self, model, device):\n",
        "        self.model = pretrained_models(model=model)\n",
        "        self.model = self.model.to(device=device)\n",
        "        self._active = False\n",
        "\n",
        "    def init(self,\n",
        "             start_im,\n",
        "             example,\n",
        "             show_R,\n",
        "             show_t):\n",
        "        self._active = True\n",
        "        self.step = 0\n",
        "\n",
        "        batch = self.batch = default_collate([example])\n",
        "        batch[\"R_rel\"] = show_R[None,...]\n",
        "        batch[\"t_rel\"] = show_t[None,...]\n",
        "\n",
        "        _, cdict, edict = self.model.get_xce(batch)\n",
        "        for k in cdict:\n",
        "            cdict[k] = cdict[k].to(device=self.model.device)\n",
        "        for k in edict:\n",
        "            edict[k] = edict[k].to(device=self.model.device)\n",
        "\n",
        "        quant_d, quant_c, dc_indices, embeddings = self.model.get_normalized_c(cdict,edict,fixed_scale=True)\n",
        "\n",
        "        start_im = start_im[None,...].to(self.model.device).permute(0,3,1,2)\n",
        "        quant_c, c_indices = self.model.encode_to_c(c=start_im)\n",
        "        cond_rec = self.model.cond_stage_model.decode(quant_c)\n",
        "\n",
        "        self.current_im = cond_rec.permute(0,2,3,1)[0]\n",
        "        self.current_sample = c_indices\n",
        "\n",
        "        self.quant_c = quant_c # to know shape\n",
        "        # for sampling\n",
        "        self.dc_indices = dc_indices\n",
        "        self.embeddings = embeddings\n",
        "\n",
        "    def __call__(self):\n",
        "        if self.step < self.current_sample.shape[1]:\n",
        "            z_start_indices = self.current_sample[:, :self.step]\n",
        "            temperature=None\n",
        "            top_k=250\n",
        "            callback=None\n",
        "            index_sample = self.model.sample(z_start_indices, self.dc_indices,\n",
        "                                             steps=1,\n",
        "                                             temperature=temperature if temperature is not None else 1.0,\n",
        "                                             sample=True,\n",
        "                                             top_k=top_k if top_k is not None else 100,\n",
        "                                             callback=callback if callback is not None else lambda k: None,\n",
        "                                             embeddings=self.embeddings)\n",
        "            self.current_sample = torch.cat((index_sample,\n",
        "                                             self.current_sample[:,self.step+1:]),\n",
        "                                            dim=1)\n",
        "\n",
        "            sample_dec = self.model.decode_to_img(self.current_sample,\n",
        "                                                  self.quant_c.shape)\n",
        "            self.current_im = sample_dec.permute(0,2,3,1)[0]\n",
        "            self.step += 1\n",
        "\n",
        "        if self.step >= self.current_sample.shape[1]:\n",
        "            self._active = False\n",
        "\n",
        "        return self.current_im\n",
        "\n",
        "    def active(self):\n",
        "        return self._active\n",
        "\n",
        "    def reconstruct(self, x):\n",
        "        x = x.to(self.model.device).permute(0,3,1,2)\n",
        "        quant_c, c_indices = self.model.encode_to_c(c=x)\n",
        "        x_rec = self.model.cond_stage_model.decode(quant_c)\n",
        "        return x_rec.permute(0,2,3,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuKVvbIMcgHD"
      },
      "source": [
        "Everything is defined. Load included example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvxlLaA2bB__"
      },
      "source": [
        "import importlib.resources as pkg_resources\n",
        "with pkg_resources.path(\"geofree.examples\", \"artist.jpg\") as path:\n",
        "  example = load_as_example(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERMV5ddyecM0"
      },
      "source": [
        "Initialize models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7N27V4YeGPW"
      },
      "source": [
        "from geofree.modules.warp.midas import Midas\n",
        "\n",
        "model = \"re_impl_depth\"\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Warning: Running on CPU---sampling might take a while...\")\n",
        "    device = torch.device(\"cpu\")\n",
        "midas = Midas().eval().to(device)\n",
        "renderer = Renderer(model=model, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f088sOjex9M"
      },
      "source": [
        "Backend for interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz7HI7r0Fo0V"
      },
      "source": [
        "class Looper(object):\n",
        "    def __init__(self, midas, renderer, example):\n",
        "        self.midas = midas\n",
        "        self.renderer = renderer\n",
        "        self.init_example(example)\n",
        "        self.RENDERING = False\n",
        "  \n",
        "    def init_example(self, example):\n",
        "        self.example = example\n",
        "\n",
        "        ims = example[\"src_img\"][None,...]\n",
        "        K = example[\"K\"]\n",
        "\n",
        "        # compute depth for preview\n",
        "        dms = [None]\n",
        "        for i in range(ims.shape[0]):\n",
        "            midas_in = torch.tensor(ims[i])[None,...].permute(0,3,1,2).to(device)\n",
        "            scaled_idepth = self.midas.fixed_scale_depth(midas_in, return_inverse_depth=True)\n",
        "            dms[i] = 1.0/scaled_idepth[0].cpu().numpy()\n",
        "\n",
        "        # now switch to pytorch\n",
        "        src_ims = torch.tensor(ims, dtype=torch.float32)\n",
        "        src_dms = torch.tensor(dms, dtype=torch.float32)\n",
        "        K = torch.tensor(K, dtype=torch.float32)\n",
        "\n",
        "        self.src_ims = src_ims.to(device=device)\n",
        "        self.src_dms = src_dms.to(device=device)\n",
        "        self.K = K.to(device=device)\n",
        "\n",
        "        self.init_cam()\n",
        "\n",
        "    def init_cam(self):\n",
        "        self.camera_pos = np.array([0.0, 0.0, 0.0])\n",
        "        self.camera_dir = np.array([0.0, 0.0, 1.0])\n",
        "        self.camera_up = np.array([0.0, 1.0, 0.0])\n",
        "        self.CAM_SPEED = 0.25\n",
        "        self.MOUSE_SENSITIVITY = 10.0\n",
        "\n",
        "    def update_camera(self, keys):\n",
        "        ######### Camera\n",
        "        if keys[\"a\"]:\n",
        "            self.camera_pos += self.CAM_SPEED*normalize(np.cross(self.camera_dir, self.camera_up))\n",
        "        if keys[\"d\"]:\n",
        "            self.camera_pos -= self.CAM_SPEED*normalize(np.cross(self.camera_dir, self.camera_up))\n",
        "        if keys[\"w\"]:\n",
        "            self.camera_pos += self.CAM_SPEED*normalize(self.camera_dir)\n",
        "        if keys[\"s\"]:\n",
        "            self.camera_pos -= self.CAM_SPEED*normalize(self.camera_dir)\n",
        "        if keys[\"q\"]:\n",
        "            self.camera_pos -= self.CAM_SPEED*normalize(self.camera_up)\n",
        "        if keys[\"e\"]:\n",
        "            self.camera_pos += self.CAM_SPEED*normalize(self.camera_up)\n",
        "\n",
        "        camera_yaw = 0\n",
        "        camera_pitch = 0\n",
        "        if \"look\" in keys:\n",
        "            dx, dy = keys[\"look\"]\n",
        "            if not self.RENDERING:\n",
        "                camera_yaw -= self.MOUSE_SENSITIVITY*dx\n",
        "                camera_pitch += self.MOUSE_SENSITIVITY*dy\n",
        "\n",
        "        # adjust for yaw and pitch\n",
        "        rotation = np.array([[cosd(-camera_yaw), 0.0, sind(-camera_yaw)],\n",
        "                             [0.0, 1.0, 0.0],\n",
        "                             [-sind(-camera_yaw), 0.0, cosd(-camera_yaw)]])\n",
        "        self.camera_dir = rotation@self.camera_dir\n",
        "\n",
        "        rotation = rotate_around_axis(camera_pitch, np.cross(self.camera_dir,\n",
        "                                                             self.camera_up))\n",
        "        self.camera_dir = rotation@self.camera_dir\n",
        "\n",
        "        show_R, show_t = look_to(self.camera_pos, self.camera_dir, self.camera_up) # look from pos in direction dir\n",
        "        show_R = torch.as_tensor(show_R, dtype=torch.float32)\n",
        "        show_t = torch.as_tensor(show_t, dtype=torch.float32)\n",
        "\n",
        "        self.show_R = show_R\n",
        "        self.show_t = show_t\n",
        "\n",
        "    def update(self, keys):\n",
        "        self.update_camera(keys)\n",
        "        if not self.RENDERING:\n",
        "            wrp_im = render_forward(self.src_ims, self.src_dms,\n",
        "                                    self.show_R, self.show_t,\n",
        "                                    K_src=self.K,\n",
        "                                    K_dst=self.K)\n",
        "            \n",
        "        if keys[\"render\"]:\n",
        "            self.RENDERING = True\n",
        "            self.renderer.init(wrp_im, self.example, self.show_R, self.show_t)\n",
        "\n",
        "        if self.RENDERING:\n",
        "            wrp_im = self.renderer()\n",
        "        \n",
        "        if not self.renderer._active or keys[\"stop\"]:\n",
        "          self.RENDERING = False\n",
        "\n",
        "        return wrp_im, self.RENDERING"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFHXmjkphpAZ"
      },
      "source": [
        "Frontend inspired by [Infinite Nature](https://infinite-nature.github.io/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXq8YBdwyTQ8"
      },
      "source": [
        "import IPython\n",
        "from google.colab import output, files\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "looper = Looper(midas, renderer, example)\n",
        "\n",
        "def as_png(x):\n",
        "  if hasattr(x, \"detach\"):\n",
        "      x = x.detach().cpu().numpy()\n",
        "  #x = x.transpose(1,0,2)\n",
        "  x = (x+1.0)*127.5\n",
        "  x = x.clip(0, 255).astype(np.uint8)\n",
        "  data = io.BytesIO()\n",
        "  Image.fromarray(x).save(data, format=\"png\")\n",
        "  data.seek(0)\n",
        "  data = data.read()\n",
        "  return base64.b64encode(data).decode()\n",
        "\n",
        "def pyloop(data):\n",
        "  if data.get(\"upload\", False):\n",
        "    data = files.upload()\n",
        "    fname = sorted(data.keys())[0]\n",
        "    I = Image.open(BytesIO(data[fname]))\n",
        "    looper.init_example(load_im_as_example(I))\n",
        "\n",
        "  keys = dict()\n",
        "  if \"look\" in data:\n",
        "    keys[\"look\"] = np.array(data[\"look\"])*2.0-1.0\n",
        "  move = data.get(\"direction\", None)\n",
        "  keys[\"w\"] = move==\"forward\"\n",
        "  keys[\"a\"] = move==\"left\"\n",
        "  keys[\"s\"] = move==\"backward\"\n",
        "  keys[\"d\"] = move==\"right\"\n",
        "  keys[\"q\"] = move==\"up\"\n",
        "  keys[\"e\"] = move==\"down\"\n",
        "  keys[\"render\"] = move==\"render\"\n",
        "  keys[\"stop\"] = data.get(\"stop\", False)\n",
        "  output, rendering = looper.update(keys)\n",
        "\n",
        "  ret = dict()\n",
        "  ret[\"image\"] = as_png(output)\n",
        "  ret[\"loop\"] = rendering\n",
        "  ret = IPython.display.JSON(ret)\n",
        "\n",
        "  return ret\n",
        "\n",
        "output.register_callback('pyloop', pyloop)\n",
        "\n",
        "# The front-end for our interactive demo.\n",
        "\n",
        "html='''\n",
        "<style>\n",
        "#view {\n",
        "  width: 368px;\n",
        "  height: 208px;\n",
        "  background-color: #aaa;\n",
        "  background-size: 100% 100%;\n",
        "  border: 1px solid #000;\n",
        "  margin: 20px;\n",
        "  position: relative;\n",
        "}\n",
        ".buttons {\n",
        "  margin: 20px;\n",
        "}\n",
        ".buttons div {\n",
        "  display: inline-block;\n",
        "  cursor: pointer;\n",
        "  padding: 20px;\n",
        "  background: #eee;\n",
        "  border: 2px solid #aaa;\n",
        "  border-radius: 3px;\n",
        "  margin-right: 10px;\n",
        "  font-weight: bold;\n",
        "  text-transform: uppercase;\n",
        "  letter-spacing: 1px;\n",
        "  color: #444;\n",
        "  width: 100px;\n",
        "  text-align: center;\n",
        "}\n",
        ".buttons div:active {\n",
        "  background: #444;\n",
        "  color: #fff;\n",
        "}\n",
        "#rgb {\n",
        "  height: 100%;\n",
        "}\n",
        "h3 {\n",
        "  margin-left: 20px;\n",
        "}\n",
        "</style>\n",
        "<h3>Braindance Colab Demo</h3>\n",
        "<div id=view><img id=rgb></div>\n",
        "<div class=buttons>\n",
        "<div id=up>Up</div><div id=forward>Forward</div><div id=down>Down</div><br>\n",
        "<div id=left>Left</div><div id=backward>Backward</div><div id=right>Right</div><br>\n",
        "<div id=render>Render</div><div id=stop>Stop</div><div id=upload>Upload</div>\n",
        "<script>\n",
        "stop_rendering = false;\n",
        "\n",
        "async function loop(...parms) {\n",
        "  result = await google.colab.kernel.invokeFunction('pyloop', parms, {});\n",
        "  result = result.data['application/json'];\n",
        "  image = result['image'];\n",
        "  // console.log(image);\n",
        "  const url = `data:image/png;base64,${image}`;\n",
        "  document.querySelector('#rgb').src = url;\n",
        "  if(stop_rendering) {\n",
        "    result['loop'] = false;\n",
        "    await google.colab.kernel.invokeFunction('pyloop', [{\"stop\": true}], {});\n",
        "  }\n",
        "  stop_rendering = false;\n",
        "  if(result['loop']) {\n",
        "    loop({});\n",
        "  }\n",
        "}\n",
        "\n",
        "function cursor(e) {\n",
        "  x = e.offsetX / e.target.clientWidth;\n",
        "  y = e.offsetY / e.target.clientHeight;\n",
        "  loop({\"look\": [x,y]})\n",
        "}\n",
        "\n",
        "function move(direction) {\n",
        "  loop({\"direction\": direction})\n",
        "}\n",
        "\n",
        "loop({});\n",
        "document.querySelector('#view').addEventListener('click', cursor);\n",
        "document.querySelector('#up').addEventListener('click', () => move(\"up\"));\n",
        "document.querySelector('#forward').addEventListener('click', () => move(\"forward\"));\n",
        "document.querySelector('#down').addEventListener('click', () => move(\"down\"));\n",
        "document.querySelector('#left').addEventListener('click', () => move(\"left\"));\n",
        "document.querySelector('#backward').addEventListener('click', () => move(\"backward\"));\n",
        "document.querySelector('#right').addEventListener('click', () => move(\"right\"));\n",
        "document.querySelector('#render').addEventListener('click', () => move(\"render\"));\n",
        "document.querySelector('#stop').addEventListener('click', () => {stop_rendering=true;});\n",
        "document.querySelector('#upload').addEventListener('click', () => loop({\"upload\": true}));\n",
        "\n",
        "\n",
        "\n",
        "</script>\n",
        "'''\n",
        "\n",
        "display(IPython.display.HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpdGtv-WhWBw"
      },
      "source": [
        "Click on the image to look around. Use the first six buttons to move around. Render to start rendering the novel view with GeoGPT. Stop to abort rendering and regain control. Upload to upload your own images. Have fun!"
      ]
    }
  ]
}